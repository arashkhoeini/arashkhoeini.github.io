[ { "title": "Understanding GARCH Models, A Beginner-Friendly Guide with Python Implementation", "url": "/posts/garch/", "categories": "insight", "tags": "artificial intelligence, Finance", "date": "2025-01-23 22:30:00 -0800", "snippet": "Have you ever noticed that stock prices or exchange rates tend to behave in clusters? For example, periods of calm with small price changes are often followed by periods of high activity with big jumps. This phenomenon, called volatility clustering, is common in financial data. To model and predict these fluctuations, we use something called a GARCH model.In this blog post, I’ll break down what GARCH models are, why they’re important, and how you can build one using Python.What is a GARCH Model?GARCH stands for Generalized Autoregressive Conditional Heteroskedasticity. While that sounds like a mouthful, it boils down to this: Time-Varying Volatility: Unlike basic models that assume constant variance, GARCH assumes that the volatility of a series changes over time. Conditional: Today’s volatility depends on past data, such as past returns or errors. Autoregressive and Moving Average: It combines two components: Past variances (autoregressive part). Past squared errors (moving average part). In simpler terms, GARCH models try to predict how volatile a time series (like stock prices) will be tomorrow, based on past behavior.Why Do We Need GARCH Models?Let’s say you’re a financial analyst managing a portfolio. To make good decisions, you need to know the risk associated with your investments. Risk is tied to volatility, and if volatility keeps changing (as it often does in markets), you can’t assume it’s constant.GARCH models help: Forecast Volatility: How bumpy will the market be tomorrow? Manage Risk: Estimate Value at Risk (VaR) to prepare for potential losses. Optimize Portfolios: Allocate your investments better by understanding future risks.How Does GARCH Work?Let’s break it into simple steps: Start with Returns: For financial time series, you first calculate daily or weekly returns. Returns are typically more predictable than raw prices. Model the Mean: Many time series models focus on predicting the average value. GARCH goes a step further and models the variance (how much the data varies around the mean). Lagged Effects: GARCH assumes today’s volatility depends on: Past squared errors (big shocks lead to high volatility later). Past variances (high volatility tends to persist over time). The GARCH FormulaThe GARCH model predicts the variance ($ \\sigma_t^2 $) of the series at time $ t $ using:\\(\\sigma_t^2 = \\omega + \\sum_{i=1}^q \\alpha_i \\epsilon_{t-i}^2 + \\sum_{j=1}^p \\beta_j \\sigma_{t-j}^2\\)Where: $ \\omega $: Baseline variance (a constant). $ \\epsilon_{t-i}^2 $: Past squared errors. These errors are the different between the predicted return and the actual return. in case of a simple GARCH model we consider the predicted return to be the average return which we assume is zero. Therefore this errors are considerend to be just the returns! $ \\sigma_{t-j}^2 $: Past variances (how volatile the past has been). $ \\alpha_i $ and $ \\beta_j $: Weights assigned to past errors and variances.We optimize the weights $\\alpha$ and $\\beta$ using Maximum Likelihood Estimation (MLE).Let’s Implement GARCH in PythonWe’ll use the arch package, a popular Python library for working with volatility models. Let’s dive into the code!Step 1: Install Required Librariespip install arch pandas matplotlibStep 2: Import Librariesimport numpy as npimport pandas as pdimport matplotlib.pyplot as pltfrom arch import arch_modelStep 3: Generate or Load DataHere, we’ll generate synthetic data that mimics financial returns. You can also load real-world data, like stock prices, using APIs like Yahoo Finance.# Generate synthetic returns datanp.random.seed(42)n = 1000 # Number of data pointsreturns = np.random.normal(loc=0, scale=1, size=n) # Simulated daily returnsStep 4: Fit a GARCH(1,1) ModelThe GARCH(1, 1) model is one of the most commonly used models, with one lag for both errors and variances.# Fit a GARCH(1, 1) modelmodel = arch_model(returns, vol=&#39;Garch&#39;, p=1, q=1, mean=&#39;Zero&#39;, dist=&#39;Normal&#39;)results = model.fit()# Print the summaryprint(results.summary())Step 5: Plot the ResultsLet’s visualize the conditional volatility predicted by the model.# Get conditional volatility (sigma_t)volatility = results.conditional_volatility# Plot the resultsplt.figure(figsize=(10, 6))plt.plot(volatility, label=&#39;Conditional Volatility&#39;, color=&#39;blue&#39;)plt.title(&#39;GARCH(1,1) Conditional Volatility&#39;)plt.xlabel(&#39;Time&#39;)plt.ylabel(&#39;Volatility&#39;)plt.legend()plt.show()Interpreting the Output Model Summary: The summary provides estimated coefficients ($ \\omega, \\alpha_1, \\beta_1 $). If $ \\alpha_1 + \\beta_1 $ is close to 1, volatility tends to persist for a long time. Volatility Plot: The conditional volatility plot shows how predicted volatility changes over time. Peaks indicate periods of high uncertainty. Real-World ExampleFor real data, you can use historical stock prices. Here’s how you can load data using Yahoo Finance:import yfinance as yf# Load stock datadata = yf.download(&#39;AAPL&#39;, start=&#39;2020-01-01&#39;, end=&#39;2023-01-01&#39;)returns = 100 * data[&#39;Adj Close&#39;].pct_change().dropna() # Calculate daily returns# Fit the GARCH modelmodel = arch_model(returns, vol=&#39;Garch&#39;, p=1, q=1, mean=&#39;Zero&#39;, dist=&#39;Normal&#39;)results = model.fit()print(results.summary())ConclusionGARCH models are powerful tools for understanding and predicting volatility in time series data. While they’re widely used in finance, they’re also applicable in areas like weather forecasting or any domain where variability matters.If you’re just starting out, try experimenting with different datasets and GARCH configurations to see how well they capture volatility patterns." }, { "title": "Exploring AI for Finance, A New Chapter in My Research Journey", "url": "/posts/ai-in-finance/", "categories": "insight", "tags": "artificial intelligence, Finance", "date": "2025-01-03 17:30:00 -0800", "snippet": "As an AI researcher, I’ve always been fascinated by how machine learning and artificial intelligence can solve complex, real-world problems. Recently, I’ve found myself drawn to a new area of exploration: AI for finance. It’s a topic that blends advanced algorithms with the dynamic, high-stakes world of financial markets, and I think there’s a lot of potential here to make a meaningful impact.Why Finance?At first glance, it might seem like a big leap from the kinds of AI applications I usually work on. But finance is essentially a data-rich environment, and as we know, data is the lifeblood of AI. From predicting stock prices to identifying fraud, there are countless opportunities to apply machine learning models. I find this field exciting because it combines technical challenges with practical applications that can affect everyday lives, businesses, and even economies.How Can AI Help in Finance?AI has the power to transform finance in profound ways. For example: Investment Strategies: AI can analyze enormous datasets at lightning speed, uncovering patterns that humans might miss. Fraud Detection: AI models can spot unusual transactions and flag them before they cause harm. Customer Service: AI-powered chatbots are revolutionizing the way banks interact with their customers.What excites me most, though, is the potential for AI to bring more transparency and fairness to the financial system. For instance, algorithms can help lenders assess creditworthiness without biases, making loans accessible to more people. AI can also improve risk management, ensuring that businesses and individuals are better prepared for uncertainties.The Role of Agentic AI in FinanceOne particularly intriguing concept is the use of agentic AI in finance. Agentic AI refers to systems capable of taking autonomous actions to achieve specific goals within defined boundaries. In finance, such systems could: Execute Trades Autonomously: AI agents can monitor markets in real-time and execute trades based on pre-set strategies or dynamic market conditions. Optimize Portfolios: Intelligent agents could automatically adjust investment portfolios to balance risk and reward, responding to market shifts with minimal human intervention. Personalized Financial Advisors: Agentic AI could serve as virtual financial advisors, helping individuals create and maintain financial plans tailored to their specific goals and constraints.What makes agentic AI especially powerful is its ability to learn and adapt over time. For example, a trading agent could refine its strategy based on market trends or feedback, becoming more effective as it gains experience.While agentic AI opens up exciting possibilities, it also raises important questions about accountability, transparency, and fairness. These are issues I’ll be exploring further in my research and writing.What’s Next?As I dive deeper into this field, I’ll be sharing my thoughts, discoveries, and projects here on this blog. There’s so much to explore—whether it’s technical aspects, ethical considerations, or real-world use cases—and I’m eager to take you along on this journey.Stay tuned for more posts on AI in finance. If you’ve been curious about this topic or have insights to share, I’d love to hear from you. Let’s explore this fascinating field together!" }, { "title": "The Bitter Truth About AI; Why Human Ingenuity Often Loses to Computation", "url": "/posts/the-bitter-lession/", "categories": "insight", "tags": "artificial intelligence, machine Learning, computation", "date": "2024-02-02 15:00:00 -0800", "snippet": "This blog post is written based on Rich Sutton’s article, “The Bitter Lesson”.Ah, the fascinating world of artificial intelligence! It’s a place where robots don’t exactly plot world domination (we hope) but are, instead, honing their skills in chess, Go, and even understanding human speech. Let’s dive into the AI rabbit hole through Rich Sutton’s thought-provoking article, “The Bitter Lesson,” and explore why brute computational force often trumps the clever tricks of human knowledge.The Battle of Brain vs. Brawn in AIImagine two grandmasters preparing for a chess match. One, the traditionalist, relies on centuries of human knowledge, classic strategies, and a refined understanding of the game. The other? A computer that doesn’t even know who Bobby Fischer is but has computational horsepower that makes your gaming PC look like a toaster. Spoiler alert: The toaster wins.Sutton’s “Bitter Lesson” tells us that in AI, it’s not about how much we know; it’s about how much computation we can throw at a problem. This isn’t just a lesson—it’s a bitter one for many researchers who spent years trying to encode human expertise into their AI systems.The Chess CheckmateIn 1997, IBM’s Deep Blue used raw computational power to defeat Garry Kasparov, the reigning world chess champion. The chess community was aghast. “How could a machine with no understanding of chess beat a human?” they cried. The answer was simple: Deep Blue didn’t need to understand. It brute-forced its way through millions of possible moves, while human-centric methods, which tried to mimic human thought processes, fell by the wayside.Go-ing Down the Same PathFast forward 20 years to the game of Go. The human brainiacs initially tried to crack it using their knowledge of the game’s intricate patterns. Enter AlphaGo, with its ability to learn from playing against itself and analyze countless positions per second. It wasn’t long before AlphaGo humbled the best human players, proving once again that sheer computational power coupled with self-learning beats human-like strategies.Talking the Talk: Speech RecognitionIn the 1970s, speech recognition researchers faced a similar dilemma. Some focused on leveraging human knowledge—understanding phonemes, the human vocal tract, and so on. Others turned to statistical methods and raw computation. The statisticians won, and today, deep learning algorithms that chew through vast amounts of data power your favorite voice assistants.Seeing is Believing: Computer VisionEarly computer vision techniques were all about identifying edges, shapes, and specific features. Then came the era of convolutional neural networks (CNNs), which don’t bother with such detailed human-like analysis. Instead, they use massive amounts of data and computation to recognize patterns. The result? Today’s computer vision systems can detect everything from cats to cancer cells with astonishing accuracy.The Bitter LessonRich Sutton’s “Bitter Lesson” is this: Human knowledge, while valuable, often complicates AI methods and limits their potential. On the other hand, general methods that scale with computational power, like search and learning, continue to improve as our computational abilities grow.It’s a bit like being told that your years of piano lessons are no match for a robot that can play all of Beethoven’s sonatas after scanning sheet music at lightning speed. Ouch.Two Key Takeaways The Power of General Methods: Methods that can leverage massive computation—like search algorithms and machine learning—tend to be more successful in the long run. They don’t rely on the nuanced, often messy human understanding of specific domains. Embrace Complexity: The actual contents of our minds and the world are incredibly complex. Rather than trying to build AIs that mimic our thinking, we should develop systems that can handle this complexity through their own processes. Conclusion: Embrace the ComputationSo, what’s the takeaway for us mere mortals? As we advance in the field of AI, we should focus less on trying to teach machines to think like us and more on building systems that can learn, adapt, and brute-force their way through problems. It’s a humbling but necessary adjustment—one that promises exciting, if occasionally bitter, progress.Remember, next time you lose to your phone’s chess app, it’s not just because it’s smarter. It’s because it has learned the “bitter lesson” and harnessed the relentless power of computation.For a deeper dive, check out Rich Sutton’s full article, “The Bitter Lesson”." }, { "title": "FTTransformer; Transformer Architecture for Tabular Datasets", "url": "/posts/FTTransformer/", "categories": "Paper", "tags": "deep learning, transformers, tabular datasets", "date": "2024-02-02 15:00:00 -0800", "snippet": "IntroductionIf you follow my blog, you’ve probably noticed my keen interest in deep learning for tabular data. It’s not because I find tasks like predicting housing prices fascinating—I don’t! My passion lies in the potential of machine learning to revolutionize personalized medicine. Imagine using a patient’s unique genomic data to provide accurate diagnoses and prognoses. Human genomics is a vast, complex field that exceeds our current understanding. That’s where machines come in—they can tackle these challenges that humans alone can’t fully grasp. Genomic data, with its high-dimensional complexity, often stumps traditional machine learning methods like decision trees. That’s why I advocate for deep learning in tabular data, and why I’m excited to introduce a transformer-based architecture designed specifically for tabular datasets.The FTTransformer, introduced in the paper Revisiting Deep Learning Models for Tabular Data, represents an innovative approach to leveraging Transformer architecture for tabular data processing. Traditionally, deep learning models for tabular data have relied on architectures like neural networks or gradient boosting machines. However, the Transformer model, known for its success in natural language processing tasks, has shown promise in handling sequential data due to its self-attention mechanism.Understanding Transformers in Tabular DataTransformers, originally designed for sequential tasks, excel at capturing relationships between elements in a sequence. In tabular data, each row can be seen as a sequence of features, where relationships between features (columns) are crucial for effective modeling. The FTTransformer adapts the Transformer architecture to learn these feature relationships directly from tabular data.Key Components of FTTransformer Input Embeddings: Just like in NLP tasks where words are embedded into vectors, FTTransformer starts by embedding each categorical feature and numerical feature into continuous vector representations. Categorical features are typically one-hot encoded and then embedded, while numerical features can be normalized and directly embedded. Transformer Encoder Layers: The core of the FTTransformer consists of multiple Transformer encoder layers. Each layer has two main components: multi-head self-attention and feed-forward neural networks. Multi-head self-attention allows the model to attend to different parts of the input sequence, capturing dependencies between features. Feed-forward neural networks process the output of the attention mechanism to generate feature-wise transformations. Feature-wise Transformation: Instead of operating on the sequence as a whole, the FTTransformer focuses on transforming each feature independently across all rows. This approach enables the model to learn feature interactions and dependencies in a more direct and interpretable manner. Output Layer: After processing through multiple encoder layers, the transformed features are aggregated and fed into an output layer. This layer produces predictions or classifications based on the transformed features, depending on the task (regression, classification, etc.). Advantages of FTTransformer Interpretable Feature Interactions: By focusing on feature-wise transformations, FTTransformer provides insights into how different features interact and contribute to predictions. Scalability: Transformers are inherently parallelizable, making FTTransformer suitable for large-scale tabular datasets with many features. Generalization: The self-attention mechanism allows the model to capture complex relationships between features without requiring explicit feature engineering. Applications and Future DirectionsThe FTTransformer opens up new avenues for applying Transformer architectures beyond natural language processing. Its potential applications include: Financial Forecasting: Predicting stock prices based on historical market data. Healthcare Analytics: Diagnosing diseases from patient records. E-commerce: Personalizing recommendations based on user behavior.Future research could focus on optimizing FTTransformer for specific tabular data domains, enhancing its interpretability, and integrating it with other machine learning techniques for improved performance.ConclusionIn conclusion, the FTTransformer represents a significant advancement in deep learning models for tabular data. By adapting Transformer architecture to handle tabular data effectively, it promises to revolutionize how we analyze and derive insights from structured datasets. As research and development in this field progress, FTTransformer is likely to become a cornerstone in the toolkit of data scientists and machine learning practitioners working with diverse and complex tabular datasets." }, { "title": "Self-Supervised Representation Learning for Tabular Datasets", "url": "/posts/self-supervision_for_tabular-data/", "categories": "Tutorial", "tags": "deep learning, self-supervised learning, tabular datasets", "date": "2022-11-18 17:30:00 -0800", "snippet": "Self-supervised Learning for Tabular DatasetsSelf-supervised learning aims to learn latent representations for unlabeled datasets. It has shown to be an effective representation learning method; even outperforming supervised representation learning in some settings, such as Transfer Learning. In the previous blog posts, I covered self-supervised learning and briefly explained various collapsing solutions. Here, I am going to write about the recent works on targeting to unleash the power of self-supervised learning for tabular datasets.The Problem With Tabular DatasetsAs I explained here, self-supervised learning can be roughly categorized into two main categories: pre-text task learning and contrastive learning (although you can see contrastive learning as a special case for pre-text task learning where the task is to learn a feature space such that map similar samples are located close to each other). Pre-text task learning defines a pre-text task, such as predicting the rotation degree of a rotated image. Contrastive learning can be defined using the notion of positive and negative pairs. For each sample x, we first create its positive pair by augmenting x, and then push the network to map x and its positive pair close to each other. To avoid collapsing solutions, we also push the network to map x far away from its negative pairs. We define negative pairs of x as all other samples plus their positive pairs.The problem with the current self-supervised approaches is that they are carefully curated for special data types. For example, pre-text task learning methods are defined based on unique characteristics of data, e.g., solving a jigsaw puzzle in computer vision or token masking in natural language processing. This problem also holds for contrastive learning, where augmentation methods are defined to create a different yet semantically similar positive pair. This is the reason why there has been a surge in proposing new self-supervised representation learning methods for tabular datasets. In this blog post, I am going to briefly introduce the main idea of three recent such papers: VIME (NeurIPS 2020), SubTab (NeurIPS 2021), and SCARF (ICLR 2022).VIMEWe all love auto-encoders, and in particular denoising auto-encoders! In denoising auto-encoders we randomly corrupt the input and train the auto-encoder to compute the reconstruction loss using the original uncorrupted data as the target, forcing our auto-encoder to learn the correlation between features and denoise our input. In VIME authors take this idea one step further and add one extra task to the network: to predict the binary mask of corrupted features.As you can see in the picture above, VIME creates a mask m for each sample x, and uses this mask to choose which features of x to corrupt. After corruption, it feeds the corrupted x into the network with two prediction heads. One head is in charge of predicting m, and the other head’s job is to predict the original uncorrupted x.SubTabSubTab is another self-supervised learning method based on pre-text task learning. The novel idea in SubTab is it trains the neural network using subsets of features, instead of all the features. As it is demonstrated in the picture above, SubTab divides each input x into multiple overlapping subsets, x1, x2, …, xn. These subsets are used to train the network with three different losses: reconstruction loss, contrastive loss, and distance loss.During inference, for each test sample x, SubTab aggregates representations of x1, x2, …, and xn by taking their average. This average acts as the x’s latent representation which we can further feed to the prediction head.SCARFSCARF is a contrastive learning method for tabular datasets. SCARF proposes a method to augment tabular data and create positive samples, which could be later used in contrastive learning. The augmentations method that SCARF proposes is as follows: For each sample x, we randomly choose some of its features. Then we replace the value of each chosen feature with the corresponding empirical marginal distribution for that feature. For example, let’s say we want to replace the value of the second feature in x. What we do is we randomly select another sample from our dataset and copy its second feature.In this article, I pointed out the problem of self-supervision on tabular datasets and introduced the core idea of three self-supervised representation learning methods tailored for tabular datasets. This is a fairly new research topic, which has been getting some attention recently. In fact, there is going to be a workshop on the topic of “Representation Learning for Tabular Data” in this year’s NeurIPS for the first time. Here is the link to the workshop and here is the list of all the accepted papers." }, { "title": "Self-supervision and Collapsing Solutions", "url": "/posts/collapsing-solutions/", "categories": "Tutorial", "tags": "deep learning, self-supervised learning", "date": "2022-07-07 20:22:00 -0700", "snippet": "Collapsing Solutions in Self-supervised LearningIn the previous post I explained how self-supervised learning has been established as a decent method for unsupervised representation learning. I dicussed pre-text task learning, contrastive learning, and touched upon a few non-contrastive learning methods.In this post, I aim to dig deeper into similarity learning. Similarity learning is a more general self-supervised learning approach and includes both contrastive and non-contrastive methods. In particular, I talk about collapsing solutions, what they are, and how different methods use different strategies to avoid these collapsing solutions.Similarity LearningSSimilarity learning is a simple concept that most of the self-supervised methods are built based on. In similarity learning, a feature extractor f learns to map similar input close to each other in the representation space. This is very intuitive and could be looked at from various levels: two photos of a same cat but with different pose should be mapped close to each other in the representation space, two photos of different cats should be mapped relatively close to each other, compared to other classes like dogs. To learn such similarities, most of the current methods use some version of Siamese networks. Siamese networks are weight-sharing networks and this makes them a natural tool for comparing two or more inputs. In practive, Siamese network is just a fancy name for feeding two different inputs through the same network twice, and computing the desired loss function using two extracted representations. The two different input samples are usually two different augmented versions of the same sample.image sourceCollapsing SolutionsThere exists one major problem with similarity learning using Siamese networks and that’s called collapsing solutions! Forcing the network to map two different images very close to each other might end up in the network learning to cheat, and map EVERY input into a one same representation. This simply happens because our network learns to create a shortcut for minimizing loss: by just mapping everything to the same input.Various strategies have been introduced in the self-supervised learning literature to avoid such collapsing shortcuts. We will discuss some of them briefly here.Contrastive LearningThe core idea behind contrastive learning is to attract the positive sample pairs and repulse the negative sample pairs. The idea is straight forward: if we want to avoid our network to map every sample into a single point (to avoid finding a collapsing solution) we need to also define negative pairs and force our model to map them far apart. Different methods have different methodologies in order to define and use negative pairs. One well-known contrastive learning method is called SimCLR and it was introduced by Geoffrey Hinton team at Google Brain. SimCLR is short for Simple Contrasting LeaRning and its simplicity truly fits its name! In SimCLR, we create an augmentation for each data point in our batch. So we will have 2N data points for batch size N. Then for each input x_i, we take its augmentations as the positive sample for x_i and train the the network to minimize the distance between their latent representations. Meanwhile, we take all 2N-2 remaining samples as the negative samples for x_i and maximize the distance between their latent representation and x_i’s. This distance maximization between each input and its negative pairs is the key in avoiding collapsing solutions.Clustering MethodsClustering methods use one distorted sample to compute ‘targets’ for the loss and another distorted version of the sample to predict these targets. This is followed by an alternate optimization (e.g. similar to k-means method) in DeepCluster or non-differentiable operators in SwAV.Asymmetry In Siamese NetworksWhen using Siamese networks for similarity learning, the main reason for collapsing is actually the symmetric architecture of the network. This symmetry exists because of the weight sharing in Siamese network. Although weight sharing is very intuitive for learning similarities, it is the driving force for collapsing. Therefore in another recent like of work new methods with asymmetric network architecture are introduced. In one method, named BYOL (Bootstrap Your Own Latent representations) two sub-networks of the Siamese network perform very different roles. Both of these sub-networks have a representation component and a projection component which projects the input image into its latent representations. But the top network (named as the online network) has one extra component compared to the bottom network (named as the target network). This extra component, which is called the prediction component, receives the latent representation of the online network as its input and tries to predict the latent representation of the target network. The other difference between these two networks is how their parameters are updated. The online network is trained through stochastic gradient descent. However, the target network is trained using the slow moving average of the online network. In another work, SimSiam, authors show that the only thing we need to prevent collapsing is a stop-grad operation. In SimSiam, the network architecture is modified to be asymmetric using a special ‘predictor’ network and the parameter updates are asymmetric such that the model parameters are only updated using one distorted version of the input, while the representations from another distorted version are used as a fixed target. Authors of SimSiam conclude that the asymmetry of the learning update, ‘stop-gradient’, is critical to preventing trivial solutions.In this blog post I explained collapsing solutions and briefly introduced three lines of research targeting to avoid collapsing solutions. Please let me know in the comments if you know any other interesting method, or if there is any particular related paper that you would like me to summarize or explain in my Medium. This is actually what I think I am going to focus more on in this blog: to explain the core idea of the interesting papers I read with a simple language. Thank you for reading :)" }, { "title": "Self-Supervised Learning", "url": "/posts/self-supervision/", "categories": "Tutorial", "tags": "deep learning, self-supervised", "date": "2021-10-24 20:22:00 -0700", "snippet": "Self-SupervisionIntroductionSelf-supervised learning (SSL) is rapidly closing the gap with supervised methods. Very recently, Facebook AI Research (FAIR), one major player in broadening the horizon of self-supervised learning, introduced SEER. SEER is a 1.3B parameter self-supervised model pre-trained on 1B Instagram images that achieves 84.2% top-1 accuracy on ImageNet, comfortably surpassing all existing self-supervised models paper. Other researchers at FAIR trained Self-Supervised Vision Transformers (SSViT) and compared it to fully supervised ViTs and convnets, and found out that SSViTs learn more powerful representations [paper].image sourceIn spite of all these recent break-throughs, the main idea behind self-supervision is not really new and it has been around for a while now, only under different names, mostly under unsupervised learning. However, there is a debate that we should stop seeing it as unsupervised, since it is not really “unsupervised” in the essence. In self-supervised learning, we are indeed supervising the model training, but with free and creative supervision signals instead of with human generated one. One very interesting and not much new example is Word2Vec paper, where we train a model to predict a word given its surrounding words. This paper came out at ICLR 2013 and the results were considered magical at that time. This paper showed that if you train such a model that tries to predict a word given its few previous and following words, the feature extractor extracts feature vectors with a lot of interesting linear relationships. For example, if we call the feature extractor f(), we can show that f(‘king’) - f(‘man’’) + f(‘woman’) = f(‘queen’). These early results showed self-supervision is fully capable of extracting semantic relationships.It is true that supervised learning has been tremendously successful during the past decade, but we all know that we cannot label everything. If we want to move towards a system which forms generalized knowledge about the world and relies on its previously acquired background knowledge of how the world works, we need something bigger than supervised learning. We need to build systems which are capable of forming some sort of common sense about the world, just like human babies. As Yann Lecun elegantly put it, this common sense is the ‘dark matter of intelligence’, and he argues that this might be learned through self-supervision here.Well if you are not already curious and motivated enough to jump head first into the realm of self-supervised learning, let me introduce one recent paper which studied and compared the latent representation learning achieved from self-supervised learning and supervised learning in a very imbalanced setting, where there is not an equal number of each class in our training dataset. Authors conduct a series of studies on the performance of self-supervised contrastive learning and supervised learning methods over multiple balanced and imbalance datasets. They show that different from supervised methods with large performance drop, the self-supervised contrastive learning methods perform stably well even when the datasets are heavily imbalanced. Their further experiments also reveal that a representation model generating a balanced feature space can generalize better than that yielding an imbalanced one. Figure below visualizes the feature space learnt by supervised learning on the left hand side, and learned by self-supervised learning on the right hand side. The dataset has been unbalanced and as you can see the supervised model has inherited this unbalancedness and learned an unbalanced feature representation, which is not the case for the self-supervised model. Authors of this paper show that a model with balanced feature space generalizes much better than its counterpart with imbalance feature space.Well, I think that’s enough for an intro. Here in this article I am going through some of the most recent and the most successful methods that use self-supervision to learn latent representations that are good enough for downstream tasks. I am planning to first go through different categories of self-supervised methods, explain briefly one or two methods in each category, and then explain the challenges in each category. At the end I am going to talk a little bit about self-supervision in Transfer Learning and more specifically, in Domain Adaptation, and introduce a recent method that uses self-supervision to better adapt to new domains.Categories of SSLRoughly speaking, current methods of SSL fall into one of these three categories: pretext task learning, contrastive learning, and non-contrastive learningPretext Task LearningPretext task learning tries to define creative pretext task problems that solving them teaches the model a good understanding of the semantics of our data. Next I am going to explain two of the most important works in pretext learning. The first one takes two patches of one image and tries to predict the relative location of the second patch to the first one. And I already spoiled the other method. It learns to solve a jigsaw puzzle. Let’s see how they achieve their goals!Context PredictionIn this paper, authors train a CNN to classify the relative position between two image patches. One tile is kept in the middle of a 3x3 grid and the other tile can be placed in any of the other available 8 locations.For example, given two above patches, the authors try to teach a model to predict 1, which is the relative position of the second patch to the first one. Or in the image below, we see that given those two images the model should predict Y=3Authors achieve their goal by training two parallel CNN-based networks with shared weights. One network takes the first patch and the other one takes the second patch as input.The architecture of the network is shown in the above picture. Each parallel network is following the AlexNet architecture as much as possible. The outputs of fc6 is then concatenated together to form the input for fc7. The output layer, fc9, has 8 neurons, each corresponding to one of 8 possible locations. Authors show that taking PASCAL challenge, their pretrained model beats AlexNet trained from scratch. However, this might not be the most fair comparison since the later one is not using any images outside of the PASCAL dataset, not just any labeled images.Solving a jigsaw puzzleThe newer version of the previous paper is this paper, which takes this to another step and use jigsaw puzzle reassembly problem as their pretext task. Authors argue that solving Jigsaw puzzles can be used to teach a system that an object is made of parts and what these parts are.Above picture shows what they tend to do. Given this tiger photo, they want to randomly extract a 3x3 patch from it, randomize the tile, and train a network capable of predicting the correct location for each tile. This is achieved by 9 parallel CNN-based networks with shared weights.Authors define 64 different permutations for each puzzle, e.g. S=(8,4,1,9,7,3,6,5,2) and the network needs to predict which one of these 64 permutations it received as input. They show that their method beats the context prediction method with a high margin in the PASCAL challenge.Contrastive LearningThe problem with pretext learning is that usually it is not easy to come up with a task that ends up in good feature extraction. There is another family of self-supervision methods –contrastive learning– which self-supervises using a different approach. Contrastive Learning tries to form positive samples for each input data x, and map x into a latent representations where x is close to its other positive mates. One big problem with this approach is called collapse, which happens when the model learns to map all inputs to an identical embedding to get the perfect result. To avoid that, we also need negative examples for each input x, which are mapped as far as possible to the feature vector of x.Above, you can see the a simple loss function for contrastive learning from this paper, where z_i is the latent representation for sample i, and sim() is a similarity function, e.g. cosine similarity. In this example, j is a positive pair for i, which is usually generated by corrupting sample i. In summary, we have N samples in each batch and we generate a positive pair for each of N samples so we have 2N samples in total. The goal is the latent representation of each sample, like z_i, to be as similar as possible to it’s positive examples, like z_j, and far from all other example in the batch and their corrupted version. So the rest of the current batch and their corrupted mate would play the role of negative samples.As you might have already noticed, there is a lot that could go wrong here. The main questions are how do we want to choose positive and negative samples? For example, in the case of using corruption to generate positive samples, what is the best choice of corruption method? Or are the other samples in the batch good enough to be the negative samples? This is the choice of these negative samples that matters the most actually. An uncareful choice could end up in a collapse, or in inaccurate modeling of the loss function. For example, in the below demonstration, green dots, which are negative paris, need to be chosen carefully in order to get a good estimation of the loss function.This a bigger issue in high-dimensional data, such as images, where there are many ways one image can be different from another. Finding a set of contrastive images that cover all the ways they can differ from a given image is a nearly impossible task. To paraphrase Leo Tolstoy’s Anna Karenina: “Happy families are all alike; every unhappy family is unhappy in its own way.” source This is why some researchers start look for novel non-contrastive methods.Non-Contrastive MethodNon-contrastive methods are probably the hottest topic in self-supervised learning. Here I will introduce two of the most recent non-contrastive SSL methods. Once from Facebook AI Research and one from Deep Mind.Barlow TwinsThe first method, which is a work of Yann Lecun research group in FAIR, owes its name to neuroscientist H. Barlow’s redundancy-reduction principle applied to a pair of identical networks. The main idea is the cross-correlation matrix between the latent representations for two different distorted versions of one image should be as close to identical as possible. Cross-correlation matrix measures the relation of one signal –here latent representation– with another one; which here means it measures the relation between each latent feature of one distorted version, with all latent features of another distorted version of the same image. Below you can see a demonstration of Barlow Twins.You might ask what is the point of having an identical cross-correlation matrix? Well, all diagonal elements of an identical matrix are one, which indicates the perfect correlation. Also all off-diagonal elements are zero, which means no correlation. In fact a diagonal cross-correlation matrix indicates that all latent variables of a same dimension have perfect correlation – similar representations for different distorted versions of a same image– and there is no redundancy between different components of latent representations.BYOLBring your own liquor! Uh no I meant Bootstrap Your Own Latent representations. This is how researchers in Deep Mind decided to call their new non-contrastive method. Similar to every other method that I proposed in this article, this one also takes advantage of two parallel networks. But this time these two networks are not symmetric, nor identical.Both of these networks receive the same input image. However, they perform very different roles. Both of these networks have a representation component and a projection component which projects the input image into its latent representations. But the top network, which is called the online network, has one extra component compared to the bottom network, which is called the target network. This extra component, which is called the prediction component, receives the latent representation of the online network as its input and tries to predict the latent representation of the target network. The other difference between these two networks is how their parameters are updated. The online network is trained through stochastic gradient descent. However, the target network is trained using the slow moving average of the online network.SSL in Domain AdaptationOne of the problems that SSL could be of some use in is Universal Domain Adaptation (UniDA). In UniDA we have a labeled source dataset from a domain, and an unlabled target dataset from another domain. Source and target dataset are sampled from different distribution and there is no constrain on their categories. This mean there might be classes in the source dataset that do not exist in the source dataset or vice versa.UniDA naturally opens an opportunity to become creative and look for different forms of self-supervision that could be useful in either the distribution matching between the source and target domains or in the open class detection, where we want to detect target data that is from a target private only calss; a class that does not exists in our source dataset. This is what Saito et.al. do in the (DANCE)[https://arxiv.org/abs/2002.07953] method.The idea behind the DANCE method is as follows: target data should either be clustered with source data so to be labelled as one of the known classes, or to form new clusters with other target data points. For this to happen, target points should either be pushed towards the current class prototypes or to get closer to the other data points. This can happen if we first define a similarity distributation for each data point, where p_ij shows how similar latent representation of point i is to F_j, where F_j is latent representation for either a class prototype or a target example. Then we can minimize the entropy of this distribution, so that we push our target latent representation towards F_j.They call this term Neighbourhood Clustering.The other term authors propose also uses entropy, but this time to align some of them with “known” source categories while keeping the “unknown” target samples far from the source. This loss term is called Entropy separation loss and we minimize it only if it’s distance from half of the max entorpy (rho) is larger than some threshold m. Please note that p is the classification output for a target sample." }, { "title": "What is a Calibrated Neural Network and why do we care?", "url": "/posts/what-is-calibration/", "categories": "Tutorial", "tags": "deep learning, uncertainty prediction, calibration", "date": "2020-12-09 00:00:00 -0800", "snippet": "We love neural networks! We use them in our phones, our home speaker, our TVs, our cars and our almost everything! We like them when they are correct, and we ignore them when they are wrong. The mercy we show when they are wrong also indicates how much we adore them! After all, it wouldn’t hurt if there are a few dog photos in the search result when you search for cats in your “Google Photos” app, right? But we cannot afford to be always this much forgiving! Sometimes we deploy neural networks in safety-critical applications such as medical diagnosis or self-driving cars. A “simple” mistake here could be catastrophic and we need to avoid that.If I train a LeNet on MNIST dataset I can get an accuracy higher than %99 on test data. But what if I deploy this network in a real-world setting, like in a post office, to read zip codes? It will obviously see digits different from what it’s used to see, digits from different distributions. For example, it might see a 5, but rotated 90degrees. In this case, our network would predict that it’s an 8 and will predict it confidently. In this setting, our neural network simply “doesn’t know that it doesn’t know” and instead of asking for expert help, it will confidently send the letter to a wrong address and some poor grandmother will never receive her Christmas card! Sad story.Our ideal neural networks are the types that produce “calibrated uncertainties”. A well-calibrated model should be accurate when it is certain about its prediction and indicate high uncertainty when it is likely to be inaccurate. What do we mean here by uncertainty? In NeurIPS2020 Google AI Brain Team provides a sweet definition. We need neural networks that return distribution over predictions rather than a single prediction. In this case, we can use distribution’s variance directly as an uncertainty estimation in regression tasks or use probability of y given x as our confidence. Good uncertainty estimates quantify when we can trust the model’s predictions, and when we should hand the control to a human expert.You might think with yourself that in the case of classification, we always have probability estimates after a final softmax layer. Can’t we just use these as our confidence in the model’s prediction? Guo et. al. show that we cannot. They show that almost all modern neural networks are miscalibrated for several reasons, such as NLL overfitting. They provide a simple and intuitive post-hoc calibration method, named temperature scaling, which is applied on a trained network to calibrate its predictions. However, Ovadia et. al. show that this method only works for i.i.d data; for data from the same distribution as the training dataset. This means a small amount of shift from training data could end up in a wrong prediction. So the poor grandmother probably won’t receive her Christmas card even if we apply temperature scaling on our model.I think we all are convinced now that we need better and more calibrated neural networks. Neural networks capable of outputting an uncertainty of their decisions. In future posts, I am planning to go through various papers and explain methods that are being used for uncertainty prediction, both baseline methods and state-of-the-art ones. But now, I would like to finish this post by the final paragraph of this excellent paper from Intel LabsAs AI systems backed by deep learning are used in safety-critical applications like autonomous vehicles, medical diagnosis, robotics etc., it is important for these systems to be explainable and trustworthy for successful deployment in real-world. Having the ability to derive uncertainty estimates provides a big step towards explainability of AI systems based on Deep Learning. Having calibrated uncertainty quantification provides grounded means for uncertainty measurement in such models. A principled way to measure reliable uncertainty is the basis on which trustworthy AI systems can be built. Research results and multiple resulting frameworks have been released for AI Fairness measurement that base components of fairness quantification on uncertainty measurements of classified output of deep learning models…… This explanation is also critical for building AI systems that are robust to adversarial blackbox and whitebox attacks. These well calibrated uncertainties can guide AI practitioners to better understand the predictions for reliable decision making, i.e. to know “when to trust” and “when not to trust” the model predictions (especially in high-risk domains like healthcare, financial, legal etc). In addition, calibrated uncertainty opens the doors for wider adoption of deep network architectures in interesting applications like multimodal fusion, anomaly detection and active learning. Using calibrated uncertainty as a measure for distributional shift (out-of-distribution and dataset shift) detection is also a key enabler for self-learning systems that form a critical component of realizing the dream of Artificial General Intelligence (AGI)." } ]
